# -*- coding: utf-8 -*-
"""BA-01/09.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uKRl1JyBEbW83ZPo3Lffv6FCvhTbCvOE
"""

import pickle

lr = pickle.load(open('lr.sav', 'rb'))

lr.predict([[22,10,2]])

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df=pd.read_csv('/content/hospital.csv',index_col=0)

df

df.describe

df.columns

display(df.dtypes)

X = df[['GENDER', 'MARITAL_STATUS', 'KEY_COMPLAINTS_CODE', 'BODY_WEIGHT',
       'BODY_HEIGHT', 'HR_PULSE', 'BP_HIGH', 'BP_LOW', 'RR',
       'PAST_MEDICAL_HISTORY_CODE', 'HB', 'UREA', 'CREATININE',
       'MODE_OF_ARRIVAL', 'STATE_AT_THE_TIME_OF_ARRIVAL', 'TYPE_OF_ADMSN',
       'TOTAL_COST_TO_HOSPITAL','CONCESSION', 'ACTUAL_RECEIVABLE_AMOUNT', 'TOTAL_LENGTH_OF_STAY',
       'LENGTH_OF_STAY_ICU', 'LENGTH_OF_STAY_WARD', 'IMPLANT_USED_',
       'COST_OF_IMPLANT']]
y = df['TOTAL_AMOUNT_BILLED_TO_THE_PATIENT']

display(X.head())
display(y.head())

display(df.corr)

numeric_df = df[['BODY_WEIGHT',
       'BODY_HEIGHT', 'HR_PULSE', 'BP_HIGH', 'BP_LOW', 'RR',
       'HB', 'UREA', 'CREATININE',
       'TOTAL_COST_TO_HOSPITAL','CONCESSION', 'ACTUAL_RECEIVABLE_AMOUNT', 'TOTAL_LENGTH_OF_STAY',
       'LENGTH_OF_STAY_ICU', 'LENGTH_OF_STAY_WARD',
       'COST_OF_IMPLANT','TOTAL_AMOUNT_BILLED_TO_THE_PATIENT']]
display(numeric_df.corr())

df.info()

import seaborn as sns

plt.figure(figsize=(12, 10))
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap of Numeric Features')
plt.show()

"""# Task
Explain how to check for multicollinearity in the dataframe `df` and provide the VIF scores for the numeric columns. Handle any missing values in the numeric columns before calculating VIF.

## Identify numeric columns

### Subtask:
Select only the numeric columns from the DataFrame `df`.

**Reasoning**:
Select numeric columns from the dataframe based on their data types.
"""

numeric_cols = df.select_dtypes(include=np.number).columns
numeric_df = df[numeric_cols]
display(numeric_df.head())

"""## Handle missing values

### Subtask:
Multicollinearity analysis (specifically VIF calculation) cannot handle missing values. We will need to address the missing values in the numeric columns.

**Reasoning**:
Inspect the numeric_df DataFrame to identify which columns contain missing values and the extent of missingness using .isnull().sum().
"""

display(numeric_df.isnull().sum())

"""**Reasoning**:
Impute the missing values in the numeric columns using the median strategy, as it is less affected by outliers compared to the mean, and then verify that all missing values have been handled.


"""

for col in numeric_df.columns:
    if numeric_df[col].isnull().any():
        median_val = numeric_df[col].median()
        numeric_df[col].fillna(median_val, inplace=True)

display(numeric_df.isnull().sum())

"""## Calculate vif

### Subtask:
Calculate the VIF for each of the numeric columns.

**Reasoning**:
Import the necessary function and calculate the VIF for each numeric column, storing the results in a new DataFrame.
"""

from statsmodels.stats.outliers_influence import variance_inflation_factor

vif_data = pd.DataFrame()
vif_data["feature"] = numeric_df.columns

vif_data["VIF"] = [variance_inflation_factor(numeric_df.values, i)
                   for i in range(len(numeric_df.columns))]

vif_data = vif_data.sort_values(by="VIF", ascending=False)
display(vif_data)

"""## Present vif scores

### Subtask:
Display the VIF scores for each numeric column to identify potential multicollinearity.

**Reasoning**:
Display the calculated VIF scores to identify multicollinearity.
"""

display(vif_data)

"""## Summary:

### Data Analysis Key Findings

*   Missing values were present in the `BP_HIGH`, `BP_LOW`, `HB`, `UREA`, and `CREATININE` columns and were imputed using the median.
*   Several numeric features exhibit very high VIF scores, indicating significant multicollinearity.
*   Features with particularly high VIF scores include `TOTAL_LENGTH_OF_STAY`, `LENGTH_OF_STAY_WARD`, `LENGTH_OF_STAY_ICU`, `BP_HIGH`, and `BP_LOW`.

### Insights or Next Steps

*   Multicollinearity can impact the reliability of regression models. Consider addressing the highly correlated features before building predictive models.
*   Potential methods to handle multicollinearity include removing highly correlated variables, combining variables, or using regularization techniques.

# Task
Create a model to predict treatment costs more accurately and sustainably.

## Data preprocessing

### Subtask:
Prepare the data for modeling, including handling categorical features and splitting the data into training and testing sets.

**Reasoning**:
Identify and one-hot encode the categorical columns, then split the data into training and testing sets for modeling.
"""

categorical_cols = df.select_dtypes(include='object').columns
df_encoded = pd.get_dummies(df, columns=categorical_cols, dummy_na=False)

X = df_encoded.drop('TOTAL_AMOUNT_BILLED_TO_THE_PATIENT', axis=1)
y = df_encoded['TOTAL_AMOUNT_BILLED_TO_THE_PATIENT']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display(X_train.head())
display(y_train.head())

"""**Reasoning**:
Check for any remaining missing values in the training and testing sets after one-hot encoding and splitting.


"""

display(X_train.isnull().sum().sum())
display(X_test.isnull().sum().sum())

"""**Reasoning**:
Impute the remaining missing values in the training and testing feature sets using the median, as this is a common strategy for handling missing values in numerical data, and then verify that no missing values remain.


"""

for col in X_train.columns:
    if X_train[col].isnull().any():
        median_val = X_train[col].median()
        X_train[col].fillna(median_val, inplace=True)
        X_test[col].fillna(median_val, inplace=True)

display(X_train.isnull().sum().sum())
display(X_test.isnull().sum().sum())

"""## Model selection

### Subtask:
Choose a suitable regression model for the prediction task.

**Reasoning**:
Based on the previous analysis which identified multicollinearity in the numeric features and the need to predict continuous treatment costs, a suitable regression model would be one that can handle multicollinearity and is effective for regression tasks. Ridge regression is a linear regression variant that incorporates L2 regularization, which helps to mitigate the effects of multicollinearity by shrinking the regression coefficients. This makes it a good choice for this problem.
"""

from sklearn.linear_model import Ridge

model = Ridge(alpha=1.0)

"""## Model training

### Subtask:
Train the selected Ridge regression model on the training data.

**Reasoning**:
Train the selected Ridge regression model using the training data.
"""

model.fit(X_train, y_train)

"""## Model evaluation

### Subtask:
Evaluate the trained Ridge regression model's performance using appropriate metrics.

**Reasoning**:
Calculate and display the MSE and R-squared scores to evaluate the model's performance on the test set.
"""

from sklearn.metrics import mean_squared_error, r2_score

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

"""## Summary:

### Data Analysis Key Findings

*   Categorical features were successfully one-hot encoded, and missing values were imputed using the median of the training data.
*   A Ridge regression model was selected for predicting treatment costs, addressing potential multicollinearity.
*   The trained Ridge regression model achieved a Mean Squared Error (MSE) of approximately 452,311,544.26 on the test set.
*   The model also achieved an R-squared score of approximately 0.951 on the test set, indicating that about 95.1% of the variance in treatment costs can be explained by the model.

### Insights or Next Steps

*   The high R-squared score suggests the Ridge regression model is a good fit for predicting treatment costs based on the provided features.
*   Further analysis could involve exploring different regression models or tuning the hyperparameters of the Ridge model to potentially improve performance and robustness.

"""

display(X.head())
display(y.head())



df1=pd.read_csv('/content/Bank Customer Churn Prediction.csv')

df1.index_col=0

df1

df1.plot.Heatmap

df1.info()

df1.describe()

df1.isnull().sum()

df1.drop((['customer_id']), axis=1, inplace=True)

df1

df1['country'].value_counts()

df1[df1.duplicated()]

df1

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df1['country'] = le.fit_transform(df1['country'])
df1
df1['gender'] = le.fit_transform(df1['gender'])
df1

df1['country'].value_counts()

X=df1.drop((['churn', 'Unnamed: 12', 'Unnamed: 13']), axis=1)
y=df1['churn']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape

X_test.shape

y_train.shape

y_test.shape

from sklearn.preprocessing import StandardScaler
std = StandardScaler()

X_train = std.fit_transform(X_train)
X_test = std.transform(X_test)

X_train

from sklearn.linear_model import LogisticRegression
cls = LogisticRegression()

cls.fit(X_train, y_train)

cls.score(X_test, y_test)

cls.score(X_train, y_train)

import pandas as pd

new_customer_data_dict={
    'credit_score':[700],
    'country':[0],
    'gender':[0],
    'age':[45],
    'tenure':[5],
    'balance':[100000.00],
    'products_number':[2],
    'credit_card':[1],
    'active_member':[1],
    'estimated_salary':[50000]
}

new_customer_data = pd.DataFrame(new_customer_data_dict)
new_customer_data = new_customer_data[X.columns]

new_customer_data

scaled_new_customer_data = std.transform(new_customer_data)

churn_prediction = cls.predict(scaled_new_customer_data)

churn_prediction

"""## Handle missing values in train and test sets

### Subtask:
Impute missing values in the training and testing sets after splitting and before scaling.

**Reasoning**:
Impute the missing values in the training and testing feature sets using the median, as this is a common strategy for handling missing values in numerical data, and then verify that no missing values remain.
"""

for col in X_train:
    if X_train[col].isnull().any():
        median_val = X_train[col].median()
        X_train[col].fillna(median_val, inplace=True)
        X_test[col].fillna(median_val, inplace=True)

display(X_train.isnull().sum().sum())
display(X_test.isnull().sum().sum())

print(f"The Prediction churn value for the new customer is: {churn_prediction[0]}")

from sklearn.metrics import confusion_matrix

y_pred=cls.predict(X_test)

confusion_matrix= confusion_matrix(y_test,y_pred)

confusion_matrix

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of the Logistic Regression model: {accuracy}")

from sklearn.model_selection import cross_val_score
cross_val_score(cls, X, y, cv=5)

cross_val_score(cls, X, y, cv=5).mean()

cls.score(X_test, y_test)

import pickle

cls = pickle.load(open('cls.sav', 'rb'))



"""## Save the trained model

### Subtask:
Save the trained logistic regression model to a file.

**Reasoning**:
Save the trained logistic regression model to a file named `cls.sav` using pickle so that it can be loaded later.
"""

import pickle

filename = 'cls.sav'
pickle.dump(cls, open(model.SAV, 'wb'))

from sklearn.metrics import f1_score

f1 = f1_score(y_test, y_pred)
print(f"F1 Score of the Logistic Regression model: {f1}")

"""## Evaluate Classification Metrics

### Subtask:
Calculate and display accuracy, precision, specificity, and sensitivity for the logistic regression model.

**Reasoning**:
Calculate accuracy, precision, sensitivity (recall), and specificity using the true and predicted values from the test set and the previously computed confusion matrix. Display these metrics to provide a comprehensive evaluation of the model's performance.
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.metrics import confusion_matrix

# Accuracy (already calculated, but recalculating for completeness)
accuracy = accuracy_score(y_test, y_pred)

# Precision
precision = precision_score(y_test, y_pred)

# Sensitivity (Recall)
sensitivity = recall_score(y_test, y_pred)

# Specificity (calculated from the confusion matrix)
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Sensitivity (Recall): {sensitivity}")
print(f"Specificity: {specificity}")



"""Here is the modified `adv_app.py` script:"""

import pickle
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# Load the trained logistic regression model
filename = 'cls.sav'
loaded_model = pickle.load(open(filename, 'rb'))

# Assume the scaler was saved after fitting to the training data
# If you haven't saved the scaler, you would need to re-fit it on your training data X_train
# For demonstration, let's assume the scaler is available.
# If not, you might need to adapt this part based on how you handle the scaler.
# Example of saving a scaler: pickle.dump(std, open('scaler.sav', 'wb'))
# Example of loading a scaler: loaded_scaler = pickle.load(open('scaler.sav', 'rb'))

# **Important:** You need to have the 'std' object (StandardScaler fitted on X_train) available
# If you saved the scaler, load it here:
# try:
#     loaded_scaler = pickle.load(open('scaler.sav', 'rb'))
# except FileNotFoundError:
#     print("Error: scaler.sav not found. Please ensure you have saved the scaler.")
#     # Or re-fit the scaler on your training data (X_train) if you have it
#     # from sklearn.preprocessing import StandardScaler
#     # std = StandardScaler()
#     # X_train = std.fit_transform(X_train_original) # Assuming X_train_original is your pre-split training data
#     # loaded_scaler = std

# For this script to run, ensure 'std' (the fitted StandardScaler) is defined before this point
# in your environment or load it from a saved file.

# Function to preprocess new customer data and make prediction
def predict_churn(credit_score, country, gender, age, tenure, balance, products_number, credit_card, active_member, estimated_salary):
    """
    Predicts churn for a new customer using the trained logistic regression model.

    Args:
        credit_score (int): Customer's credit score.
        country (int): Encoded country of the customer.
        gender (int): Encoded gender of the customer.
        age (int): Age of the customer.
        tenure (int): Tenure of the customer.
        balance (float): Customer's account balance.
        products_number (int): Number of bank products the customer uses.
        credit_card (int): Whether the customer has a credit card (1 for yes, 0 for no).
        active_member (int): Whether the customer is an active member (1 for yes, 0 for no).
        estimated_salary (float): Estimated salary of the customer.

    Returns:
        int: Predicted churn value (1 for churn, 0 for no churn).
    """
    # Create a DataFrame from the input data
    new_customer_data = pd.DataFrame({
        'credit_score': [credit_score],
        'country': [country],
        'gender': [gender],
        'age': [age],
        'tenure': [tenure],
        'balance': [balance],
        'products_number': [products_number],
        'credit_card': [credit_card],
        'active_member': [active_member],
        'estimated_salary': [estimated_salary]
    })

    # Ensure the columns are in the same order as the training data X
    # Assuming X is available in your environment or you define its columns
    # If X is not available, you might need to define the expected column order.
    try:
        new_customer_data = new_customer_data[X.columns]
    except NameError:
         print("Warning: 'X' (training data columns) not found. Assuming default column order.")
         # Define expected columns if X is not available
         expected_columns = ['credit_score', 'country', 'gender', 'age', 'tenure', 'balance', 'products_number', 'credit_card', 'active_member', 'estimated_salary']
         new_customer_data = new_customer_data[expected_columns]


    # Scale the new customer data using the *fitted* scaler
    # Make sure the 'std' object is the StandardScaler fitted on your training data
    try:
        scaled_new_customer_data = std.transform(new_customer_data)
    except NameError:
        print("Error: 'std' (StandardScaler) not found. Cannot scale new data.")
        return None # Or handle the error appropriately

    # Make prediction
    churn_prediction = loaded_model.predict(scaled_new_customer_data)

    return churn_prediction[0]

# Example usage:
# Replace with actual values for a new customer
# Note: Ensure 'country' and 'gender' are encoded as integers based on your training data
example_prediction = predict_churn(
    credit_score=700,
    country=0, # Example encoded country
    gender=0,  # Example encoded gender
    age=45,
    tenure=5,
    balance=100000.00,
    products_number=2,
    credit_card=1,
    active_member=1,
    estimated_salary=50000
)

if example_prediction is not None:
    print(f"Predicted churn for the example customer: {example_prediction}")
    if example_prediction == 1:
        print("This customer is predicted to churn.")
    else:
        print("This customer is predicted not to churn.")