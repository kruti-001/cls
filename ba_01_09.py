# -*- coding: utf-8 -*-
"""BA-01/09.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uKRl1JyBEbW83ZPo3Lffv6FCvhTbCvOE
"""



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


df1=pd.read_csv('/content/Bank Customer Churn Prediction.csv')

df1.index_col=0

df1

df1.plot.Heatmap

df1.info()

df1.describe()

df1.isnull().sum()

df1.drop((['customer_id']), axis=1, inplace=True)

df1

df1['country'].value_counts()

df1[df1.duplicated()]

df1

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df1['country'] = le.fit_transform(df1['country'])
df1
df1['gender'] = le.fit_transform(df1['gender'])
df1

df1['country'].value_counts()

X=df1.drop((['churn', 'Unnamed: 12', 'Unnamed: 13']), axis=1)
y=df1['churn']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape

X_test.shape

y_train.shape

y_test.shape

from sklearn.preprocessing import StandardScaler
std = StandardScaler()

X_train = std.fit_transform(X_train)
X_test = std.transform(X_test)

X_train

from sklearn.linear_model import LogisticRegression
cls = LogisticRegression()

cls.fit(X_train, y_train)

cls.score(X_test, y_test)

cls.score(X_train, y_train)

import pandas as pd

new_customer_data_dict={
    'credit_score':[700],
    'country':[0],
    'gender':[0],
    'age':[45],
    'tenure':[5],
    'balance':[100000.00],
    'products_number':[2],
    'credit_card':[1],
    'active_member':[1],
    'estimated_salary':[50000]
}

new_customer_data = pd.DataFrame(new_customer_data_dict)
new_customer_data = new_customer_data[X.columns]

new_customer_data

scaled_new_customer_data = std.transform(new_customer_data)

churn_prediction = cls.predict(scaled_new_customer_data)

churn_prediction

"""## Handle missing values in train and test sets

### Subtask:
Impute missing values in the training and testing sets after splitting and before scaling.

**Reasoning**:
Impute the missing values in the training and testing feature sets using the median, as this is a common strategy for handling missing values in numerical data, and then verify that no missing values remain.
"""

for col in X_train:
    if X_train[col].isnull().any():
        median_val = X_train[col].median()
        X_train[col].fillna(median_val, inplace=True)
        X_test[col].fillna(median_val, inplace=True)

display(X_train.isnull().sum().sum())
display(X_test.isnull().sum().sum())

print(f"The Prediction churn value for the new customer is: {churn_prediction[0]}")

from sklearn.metrics import confusion_matrix

y_pred=cls.predict(X_test)

confusion_matrix= confusion_matrix(y_test,y_pred)

confusion_matrix

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of the Logistic Regression model: {accuracy}")

from sklearn.model_selection import cross_val_score
cross_val_score(cls, X, y, cv=5)

cross_val_score(cls, X, y, cv=5).mean()

cls.score(X_test, y_test)

import pickle

cls = pickle.load(open('cls.sav', 'rb'))



"""## Save the trained model

### Subtask:
Save the trained logistic regression model to a file.

**Reasoning**:
Save the trained logistic regression model to a file named `cls.sav` using pickle so that it can be loaded later.
"""

import pickle

filename = 'cls.sav'
pickle.dump(cls, open(model.SAV, 'wb'))

from sklearn.metrics import f1_score

f1 = f1_score(y_test, y_pred)
print(f"F1 Score of the Logistic Regression model: {f1}")

"""## Evaluate Classification Metrics

### Subtask:
Calculate and display accuracy, precision, specificity, and sensitivity for the logistic regression model.

**Reasoning**:
Calculate accuracy, precision, sensitivity (recall), and specificity using the true and predicted values from the test set and the previously computed confusion matrix. Display these metrics to provide a comprehensive evaluation of the model's performance.
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.metrics import confusion_matrix

# Accuracy (already calculated, but recalculating for completeness)
accuracy = accuracy_score(y_test, y_pred)

# Precision
precision = precision_score(y_test, y_pred)

# Sensitivity (Recall)
sensitivity = recall_score(y_test, y_pred)

# Specificity (calculated from the confusion matrix)
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Sensitivity (Recall): {sensitivity}")
print(f"Specificity: {specificity}")



"""Here is the modified `adv_app.py` script:"""

import pickle
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# Load the trained logistic regression model
filename = 'cls.sav'
loaded_model = pickle.load(open(filename, 'rb'))

# Assume the scaler was saved after fitting to the training data
# If you haven't saved the scaler, you would need to re-fit it on your training data X_train
# For demonstration, let's assume the scaler is available.
# If not, you might need to adapt this part based on how you handle the scaler.
# Example of saving a scaler: pickle.dump(std, open('scaler.sav', 'wb'))
# Example of loading a scaler: loaded_scaler = pickle.load(open('scaler.sav', 'rb'))

# **Important:** You need to have the 'std' object (StandardScaler fitted on X_train) available
# If you saved the scaler, load it here:
# try:
#     loaded_scaler = pickle.load(open('scaler.sav', 'rb'))
# except FileNotFoundError:
#     print("Error: scaler.sav not found. Please ensure you have saved the scaler.")
#     # Or re-fit the scaler on your training data (X_train) if you have it
#     # from sklearn.preprocessing import StandardScaler
#     # std = StandardScaler()
#     # X_train = std.fit_transform(X_train_original) # Assuming X_train_original is your pre-split training data
#     # loaded_scaler = std

# For this script to run, ensure 'std' (the fitted StandardScaler) is defined before this point
# in your environment or load it from a saved file.

# Function to preprocess new customer data and make prediction
def predict_churn(credit_score, country, gender, age, tenure, balance, products_number, credit_card, active_member, estimated_salary):
    """
    Predicts churn for a new customer using the trained logistic regression model.

    Args:
        credit_score (int): Customer's credit score.
        country (int): Encoded country of the customer.
        gender (int): Encoded gender of the customer.
        age (int): Age of the customer.
        tenure (int): Tenure of the customer.
        balance (float): Customer's account balance.
        products_number (int): Number of bank products the customer uses.
        credit_card (int): Whether the customer has a credit card (1 for yes, 0 for no).
        active_member (int): Whether the customer is an active member (1 for yes, 0 for no).
        estimated_salary (float): Estimated salary of the customer.

    Returns:
        int: Predicted churn value (1 for churn, 0 for no churn).
    """
    # Create a DataFrame from the input data
    new_customer_data = pd.DataFrame({
        'credit_score': [credit_score],
        'country': [country],
        'gender': [gender],
        'age': [age],
        'tenure': [tenure],
        'balance': [balance],
        'products_number': [products_number],
        'credit_card': [credit_card],
        'active_member': [active_member],
        'estimated_salary': [estimated_salary]
    })

    # Ensure the columns are in the same order as the training data X
    # Assuming X is available in your environment or you define its columns
    # If X is not available, you might need to define the expected column order.
    try:
        new_customer_data = new_customer_data[X.columns]
    except NameError:
         print("Warning: 'X' (training data columns) not found. Assuming default column order.")
         # Define expected columns if X is not available
         expected_columns = ['credit_score', 'country', 'gender', 'age', 'tenure', 'balance', 'products_number', 'credit_card', 'active_member', 'estimated_salary']
         new_customer_data = new_customer_data[expected_columns]


    # Scale the new customer data using the *fitted* scaler
    # Make sure the 'std' object is the StandardScaler fitted on your training data
    try:
        scaled_new_customer_data = std.transform(new_customer_data)
    except NameError:
        print("Error: 'std' (StandardScaler) not found. Cannot scale new data.")
        return None # Or handle the error appropriately

    # Make prediction
    churn_prediction = loaded_model.predict(scaled_new_customer_data)

    return churn_prediction[0]

# Example usage:
# Replace with actual values for a new customer
# Note: Ensure 'country' and 'gender' are encoded as integers based on your training data
example_prediction = predict_churn(
    credit_score=700,
    country=0, # Example encoded country
    gender=0,  # Example encoded gender
    age=45,
    tenure=5,
    balance=100000.00,
    products_number=2,
    credit_card=1,
    active_member=1,
    estimated_salary=50000
)

if example_prediction is not None:
    print(f"Predicted churn for the example customer: {example_prediction}")
    if example_prediction == 1:
        print("This customer is predicted to churn.")
    else:
        print("This customer is predicted not to churn.")